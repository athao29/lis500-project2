<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Our Model - Mood Classifier</title>
  <link rel="stylesheet" href="stylepage.css" />
</head>
<body>
  <header>
    <h1>Our Model - Mood Classifier</h1>

    <nav>
        <ul class="nav-list">
            <li><a href="index.html">HOME</a></li>
            <li><a href="about.html">ABOUT US</a></li>
            <li><a href="resource.html">IMPLICIT BIAS RESOURCES</a></li>
            <li><a href="techhero.html">TECH HEROES</a></li>
            <li><a href="proj3.html">PROJECT 3</a></li>
        </ul>
    </nav>
  </header>

  <main>
    <!-- ML MODEL LINK -->
    <section id="try-model">
      <h2>Try Our Mood Classifier</h2>
      <p>Click below to open and test the model live in a new tab:</p>
      <p><a href="https://teachablemachine.withgoogle.com/models/lK_fO0BCi/" target="_blank" class="ext-link">Launch Mood Classifier</a></p>
    </section>

    <!-- ML MODEL DEMONSTRATION VIDEO -->
    <section id="video">
      <h2>Video Demonstration</h2>
      <p>A demonstration of our mood classifier in action.</p>
      <p><a href="https://youtu.be/e85jFS5tLfY?si=yrCPIzUnqiMCOMD7" class="ext-link">Link to video demonstration in a new tab</a></p>
      <div class="iframe-container">
        <iframe src="https://www.youtube.com/embed/e85jFS5tLfY?si=yrCPIzUnqiMCOMD7" title="Model Demonstration" frameborder="0" allowfullscreen></iframe>
      </div>
    </section>

    <!-- GROUP PROJECT STATEMENT -->
    <section id="project-statement">
      <h2>Project Statement – Mood Classifier</h2>
        <p>
          For this project, our team (Allyssa and Darwish) created a machine learning model using Google’s Teachable Machine to classify human emotions—specifically Happy, Tired, and Angry. 
          These three emotions were chosen because they are simple to embody while remaining complex enough to fool an algorithm. 
          In addition to making the classifier function, our primary objective was to critically examine how AI models perceive human emotions, particularly in light of Joy Buolamwini's <em>Unmasking AI</em>. 
          Reading Parts 4 to 6 while doing this assignment really opened our eyes to how messy and biased AI systems can be—sometimes in ways we don’t even realize until we’re the ones building them.
        </p>
        <p>
          First, we gathered training data. 
          We all recorded ourselves imitating the three emotions on our webcams. 
          After that, we extracted still photos with various lighting conditions, facial angles, and even other variations like spectacles. 
          After uploading those photos to Teachable Machine, we used the included MobileNet to train the model. 
          Following several iterations of testing, we exported it and used TensorFlow.js and ml5.js to put it into a website. 
          Watching the algorithm predict our mood in real time was sort of bizarre; it was almost magical, until it didn't work out so well.
        </p>
        <p>
          And that’s where things got interesting.
        </p>
        <p>
          Despite the classifier's "technical" operation, we soon discovered its shortcomings. 
          Sometimes it would call someone “happy” when they were clearly tired, or confused a resting face for anger. 
          We initially believed it to be the model's fault. 
          However, it all made perfect sense after reading Part 4 of <em>Unmasking AI</em>, particularly when Buolamwini discusses the Mirage of Neutrality. 
          Data is never unbiased. 
          The choices we made about our lighting, who we included, and how we posed were all mirrored in our model. 
          It went beyond simply identifying patterns. 
          Even when we didn't want to incorporate them, it was perpetuating our own prejudices.
        </p>
        <p>
          Our team found Buolamwini's concept of the coded gaze to be rather memorable. 
          She describes how the beliefs and viewpoints of individuals who create AI are reflected in it. 
          We witnessed this personally. 
          People who looked like us and lived in settings we were familiar with provided the majority of our training data.
           However, the model occasionally misclassified people who weren't in our group, particularly those with differing skin tones or facial traits. 
          It served as a sort of reality check. 
          If a fun classroom project could reflect bias that clearly, how much worse could it be when these systems are used at scale?
        </p>
        <p>
          In Part 5, Buolamwini discusses her Gender Shades study, in which she demonstrated how commercial facial analysis algorithms tend to fail Black women. 
          That struck a deep chord with our experience of testing. 
          Despite its simplicity, our model had indications of the same issue: it had trouble identifying faces that weren't in the training set. 
          At that point, we understood that "working well" did not equate to "working fairly." 
          Equity is not the same as accuracy.
        </p>
        <p>
          The concept of frictionless surveillance was one that truly got us wondering. 
          Buolamwini cautions about systems that appear beneficial but are actually secretly monitoring, calculating, and making judgments. 
          Although it wasn't the purpose of our model, it got us thinking: what if workplaces or schools adopted something similar? 
          Someone's behavior may suffer if the classifier categorizes them as "angry" based just on their serious resting face. 
          We were reminded by Buolamwini's criticism that AI can influence reality in addition to describing it. 
          It must be held responsible for the power it possesses.          
        </p>
        <p>
          The concept of ground truth taught us another important lesson. 
          This label—the "correct" response—is what the machine learning model learns from. 
          But when it comes to mood, what really is the right term? 
          A person is not necessarily happy just because they smile. 
          Everyone's definition of fatigue is different. 
          We came to see that we were using our presumptions to train the model rather than the objective facts. 
          That's a dangerous starting point. 
          Treating subjective data as objective can have major negative effects, as Buolamwini notes, particularly if the results are perceived as "just science."
        </p>
        <p>
          In part 6, when Buolamwini urged people to oppose dangerous technology, our perspectives on our entire mission changed. 
          She discusses algorithmic resistance, which is the act of opposing tools that do not benefit everyone equitably. 
          That got us thinking: Why are we doing this? Is this only a neat demonstration, or are we utilizing it to learn how to create more inclusive, considerate systems? 
          We considered how we were developing the model and if it was moral to distribute it widely after being asked that question. 
          We ultimately decided that if this were ever made public outside of the classroom, it should be accompanied by a concise description of its context and limits. 
          We didn't want people to take it too seriously and think that deep comprehension is the same as pattern recognition.
        </p>
        <p>
          One thing that shocked us was how much people trusted the model. 
          People frequently just accepted being called "happy" even when they weren't. 
          This is what Buolamwini refers to as the algorithm's authority, and it is true. 
          Even when technology is flawed, people still trust it. 
          Given how subjective emotion detection is already, such a type of naive confidence can be harmful.
        </p>
        <p>
          We learned the significance of diverse viewpoints while working on this project. 
          Our group disagreed on how to pose for "angry" or what "tired" should seem like. 
          These distinctions generated fascinating discussions about the model as well as how our identities affect how we engage with AI. 
          We believe Buolamwini would agree that examining not just what we're creating, but also who we are in the process, is a necessary step.
        </p>
        <p>
          By the end of the project, our team had a better understanding of how technical tools might represent societal challenges. 
          Buolamwini's reminder that AI is a human creation, influenced by our decisions, constraints, and presumptions, struck a deep chord. 
          We had control over how we gathered data, tested it, and explained what it could and could not do, even though we were using a pre-built platform like Teachable Machine.
        </p>
        <p>
          "You are not powerless" was one statement from Part 6 that truly stuck with us. 
          It is always possible to resist. 
          It served as a reminder that we are not only passive consumers of technology, but rather creators with the power to influence the future. 
          Training a classifier wasn't the only goal of this endeavor. 
          It focused on developing critical thinking skills regarding what we construct, why we build it, and who could be impacted.
        </p>
    </section>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/image@latest/dist/teachablemachine-image.min.js"></script>
    <script src="./scripts/model.js"></script>
  </main>

  <footer>
    <p>&copy; L I S 500 Project #3 - [Darwish], [Allyssa]</p>
  </footer>
</body>
</html>